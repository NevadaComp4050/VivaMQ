## Testing Criteria ##

### 1. **Generated questions test the student's Familiarity with the written text**
   - **Relevance:** Ensure that the generated questions directly relate to the key concepts presented in the student's submission.
   - **Coverage:** Ensure that the questions cover all significant concepts discussed in the submission, without overlooking major topics.
   - **Depth:** Ensure that the questions require a thorough understanding of the concepts, rather than just surface-level knowledge.

### 2. **Generated questions test the student's Discussion Proficiency**
   - **Clarity:** The questions should prompt the student to articulate their understanding clearly and concisely.
   - **Open-Ended Nature:** Ensure that the questions encourage detailed explanations rather than simple yes/no or one-word answers.
   - **Engagement:** Ensure that the questions provoke thoughtful discussion, encouraging students to elaborate on their points.

### 3. **Generated questions test the student's Critical Expansion ability**
   - **Analytical Thinking:** Ensure that the questions require students to analyze, compare, or critique the concepts they’ve presented.
   - **Creativity:** Ensure that some questions push students to think beyond the immediate content, potentially asking them to apply concepts to new scenarios or generate new ideas.
   - **Counter-arguments:** Check if the system occasionally generates questions that challenge the student’s position or argument, prompting them to defend or reconsider their stance.

### 4. **Consistency and Reliability**
   - **Consistency:** Test if similar submissions produce consistently high-quality questions that adhere to the criteria mentioned above.
   - **Variety:** Ensure that the system doesn’t generate repetitive questions for different students or across different submissions of the same student.
   - **Robustness:** Assess whether the system handles diverse submission types (e.g., essays, reports) effectively, without generating irrelevant or nonsensical questions.

### 5. **API Performance and Reliability**
   - **Response Time:** Measure the latency of the API calls and ensure the system generates questions within an acceptable timeframe.
   - **Error Handling:** Test how the system handles API errors or unexpected inputs, ensuring that it fails gracefully and provides useful feedback to the users of VivaMQ.
   - **Scalability:** Assess the system's ability to handle multiple concurrent requests, ensuring stable performance even under high load.

### 6. **Usability and User Experience**
   - **Question Clarity:** Ensure that the generated questions are easy to understand and well-formulated, with proper grammar and syntax.
   - **Student Feedback:** Implement mechanisms to gather feedback from students on the quality and relevance of the questions, and use this feedback to refine the system.
   - **Instructor Control:** Test if instructors can review and edit questions before they are presented to students, allowing for manual refinement where necessary.





